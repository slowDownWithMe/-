kafka：
	消息系统分类：
		Peer-to-Peer：
			一般基于Pull或者Polling接收消息
			发送到队列的消息被一个而且仅仅一个接收者所接受
			即使有多个接收者在同一队列中侦听同一消息
			即支持异步‘即发即弃’的消息传送方式，页支持同步请求/应答的方式
		
		发布/订阅：
			发布到一个主题的消息，可被多个订阅者所接收
			发布/订阅即可基于Push消费数据，也可基于Pull或者Polling消费数据
			解耦能比p2p模型更强
		
	消息系统适用场景：
		解耦
		冗余
		扩展
		峰值处理能力（流量削峰），
		可恢复性
		异步通信

	kafka设计目标：
		搞吞吐率：
			在廉价的商用机器上单机可支持每秒100wW条消息的读写
		消息持久化：
			所有消息军备持久化到磁盘，无消息丢失，支持消息重做
		完全分布式：
			Producer，Broker，Consumer均支持水平扩展
		同时满足适应在线流处理和离线批处理
	
	架构简介：
		zookeeper，Broker，Producer，consumer

	部署kafka：
		使用卡夫卡自带的zooKeeper
		
		使用单独的zooKeeper：

	整个架构：
		
		topic&Partition：
			
			topic:
				逻辑概念，同一个Topic的消息可分布在一个或多个节点上
				一个topic包含一个或者多个partition
				每条消息都属于且仅属于一个topic
				Producer发布数据时，必须制定该条消息发布到哪一个topic
				Consumer订阅消息时，页必须制定订阅那个Topic消息
			
			partition：
				物理概念，一个partition只分布在一个broker上，不考虑备份
				一个partition物理上对应一个文件夹
				一个partition包含读个Sehment，Segment对用户透明
				一个Segment对应一个文件
				Segment由一个个不可变记录组成
				记录只会被appendSegment中，不会被单独删除或者修改
				清楚过期日志时，直接删除一个或多个Segment
		
		Sync Producer & 	Async Producer：
			Sync Producer：
				低延迟，低吞吐率，无数据丢失
			Async Producer：
				高延迟，高吞吐率，可能会有数据丢失
		
		数据复制和failOver：
			
			一致性方案：
				Master-slave：
					RDMBS的读写分离即为典型的Master-slave
					同步复制可保证强一致性但会影响可用性
					异步复制可提供高可用性但会降低一致性
				
				WNR：
					主要用于去中心化（P2P）的分布式系统中；DynamoDB与Cassandra即采用此方案
					N代表副本数，W代表每次写操作要保证的最少成功的副本数，R代表每次读至少读取的副本数
					当W+R > N时，可保证每次读取的数据至少有一个副本具有最新的更新
					多个写操作的顺序难以保证，可能导致多副本间的写操作顺序不一致，Dynamo通过向量时钟保证最终一致性
				
				Paxos机器变种：
					Google的Chubby，Zookeeper的Zab，RAFT邓
		
		Replica：
			当某个Topic的replication-factor为N且N大于1时，每个Partition都会有N个副本（Replica）
			Replica的个数小于等于Broker数，即对每个Partition而言每个Broker上只会有一个Replica，可用Broker ID表示Replica
			所有Partition的所有Replica默认情况下回均匀分布到所有Broker上
		
		Commit：
			ISR：
				Leader会维护一个与其基本保持听不的Replica列表，该列表称为ISP（in-sync-Replica）
				如果一个Follower比Leader落后太多，或者超过一定时间发起数据复制请求，则Leader将其从ISR中移除
				当ISR中所有的Replica都向Leader发送ACK时，Leader即Commit
			

			Commit策略：
				Server配置：
					replica.lag.time.max.ms=10000
					replica.lag.max.messages=4000	
				Topic配置：
					min.insync.replicas-=1
				Producer配置：
					request.required.acks=0		
		
		
		出路Replica全部宕机：
			等待ISR中任一Replica恢复，并选他为Leader
				等待时间较长，降低可用性
				或ISR中的所有Replica都无法恢复或者数据丢失，则该Partition将融不可用
				
			选择第一个恢复的Replica为新的Leader，无论它是否在ISR中
				并包含所有已被之前Leader Commit过的消息，因此会造成数据丢失
				可用性较高
		
	安装ZooKeeper和Kafka集群：
		VM安装：
			
	
	Kafka使用zookeeper：
		配置管理：
					
		
		Leader Election：
			抢注Leader0--非公平模式：
				创建Leader父节点，如/chroot，并将其设置为persist节点
				各客户端通过在/chrppt下创建Leader节点，如/chroot/leader，来竞争Leader；改节点应被设置为ephemera
				若某创建Leader节点成功，则该客户端成功竞选Leader
				若创建Leader节点失败，则竞选Leader失败，在/chroot/leader节点上注册exist的watch，一旦该节点被删除则获得通知
				Leader可通过删除Leader节点来放弃Leader
				如果Leader宕机，由于Leader节点被设置为eohemeral，Leader节点会自行删除；而其他节点由于在leader节点上注册了watcher，谷得到通知，参与下一轮竞选，杏儿保证总有客户端已Leader角色工作		
			
			先到先得，后者监视前者--公平模式：
				创建Leader父节点，如/chroot，并将其设置为persist
				各客户端通过在/chroot下创建Leader节点，如/chroot/leader，来竞争leader;改节点应被设置为ephemeral_sequential
				客户端通过getChildren方法获取/chroot/下所有子节点，如果其注册的节点的id在所有子节点中最小，则当前客户端竞选成功
				否则，在前面一个节点上注册watch，一旦前者被删除，则得到他的通知，返回step3（并不能直接认为自己成为leader，因为前面的节点可能只是宕机了）
				Leader节点可通过自行删除自己创建的节点来放弃leader
		
		LeaderLatch（Leader Election在Curator中的实现）：
			竞选为Leader后，不可自行放弃领导权
			只能通过close方法放弃领导权
			强烈建议增加ConnectionStateListener，当连接SUSPENDED或者LOST时视为丢失领导权
			可通过await方法等待成功获取领导权，并可加入timeout
			可通过hasleadership方法判断是否为leader
			可通过getLeader方法获取当前leader
			可通过getParticipants方法获取当前竞选Leader的参与方
	
		LeaderSelector（Leader Election在Curator中的实现）：
			竞选Leader成功后回调takeLeadership方法
			可在takeLeadership方法中实现业务逻辑
			一旦takeLeadership方法返回，即视为放弃领导权
			可通过autoRequeue方法循环获取领导权
			可通过hasLeadership方法判断是否为Leader
			可通过getLeader方法获取当前Leader		
			可通过getParticipants方法获取当前竞选Leader的参与方

		
		kafka‘各自为政’LeaderElection：
			每个partition的多个Replica同时竞争Leader
			
			优点：实现简单
			缺点：herdEffrct，Zookeeper负载过重，Latency较大

		kafka基于controller的Leader Election：
			整个集群中选举出一个Broker作为controller
			Controller为所有的Topic的所有Partition指定Leader及follower
			
			优点：极大缓解Herd Effect问题，减轻zookeep-er负载，Controller与Leader及follower间通过RPC通信，高效且实时
			缺点：引入Controller增加复杂度，需要考虑Controller的Failover

		

		服务发现：


	kafka consumer：
		producer通过主动Push的方式将消息发布到Broker
		
		Consumer通过Pull从Broker消费数据
		
		push：
			优势：延时低
			劣势：可能造成Consumer来不及处理消息；网络堵塞
		pull：
			优势：Consumer按实际处理能力获取相应量的数据；Broker实现简单
			劣势：如果处理不好，实时性相对不足（kafka使用long polling）
		
		High Level Consumer：
			很多应用场景下，客户程序只是希望从Kafka顺序读取并处理数据，而不太关心具体的offset
			同事也希望提供一些语义，例如同一条消息纸杯某一个COnsumer消费或被所有的Consumer消费
			Kafka HighLevel API提供了一个从Kafka消费数据的高层抽象，从而屏蔽其中的细节，并提供丰富的语义
		

		Consumer Group：
			HighLevel Consumerer将从某个Partition读取的最后一条消息的offset存于zookeepoer中（从0.82开始同时支持将offset存于Zookeeper中和专用的Kafka Topic中）
			
			这个offset基于客户程序提供给Kafka的名字来保存，这个名字呗称为Consumer Group、
			
			Consumer Group是整个Kafka集群全局唯一的，而非针对某个Topic的
			
			每个High Level Consumer实例都属于一个COnsumer Group，若不指定则属于默认的Group
		
			消息被消费后，并不会被删除，只是相应的offset加一
			对于每条消息，在同一个Consumer Group里只会被一个Consumer消费
			不同Consumer Group可消费同一条消息

		High Level Consumer Rebalance：
			Consumer启动及Reblance过程：
				High Level Consumer 启动时将其ID注册到Consumer Group下，在zookeeper上的路径为/consumers/[consumer group]/ids/[consumer id]
				在/consumers/[consumer group]/ids上注册Watch
				在/brokers/ids上注册watch
				如果Consumer通过Topic Filter创建消息流，则它会同时在/brokers/topics上也创建Watch
				强制自己在其COnsumer Group内启动Rebalance流程
			
			Consumer Rebalance算法：
				将目标Topic下的所有Partition排序，存于Pt
				对某Consumer Group下所有Consumer排序，存于Cg，第i个Consumer记为Ci
				N=size（Pt）/size(Cg),向上取整
				接触Ci对原来分配的Partition的消费权（i从0开始）
				将第i*N到（i+1）*N-1个Partition分配给Ci

			Consumer Rebalance算法缺陷及改进：
				Herd Effect任何Broker或者Consumer的增减都会出发所有的COnsumer的Rebalance
				Split Brain每个COnsumer分别单独通过Zookper判断哪些Broker和Consumer宕机，同事Consumer在同一 刻从zookeeper看到的View可能不完全一样，这是由Zookeeper的特性决定的
				调整结果不可控，所有Consumer分别进行Rebalance，彼此不知道对应的Rebalance是否成功

		LowLevel Consumer：
			使用Low Level Consumer（Simple Consumer）的主要原因是，用户希望比COnsumer Group更好的控制数据的消费
				
				同一条消息读多次，方便reolay
				只消费某个Topic的部分Partition
				管理事务，从而确保每条消息被处理一次（Exactly once）
				
			与High Level Consumer相对，Low Level Consumer要求用户做大量的额外工作
				在应哟用程序中跟踪处理offset，并决定下一条消费那条消息
				获知每个Partition的Leader
				处理Leader的变化
				处理多Consumer的写作
			
			
	Consumer Offset Management：
		Simple Consumer：
			手工管理offset：
				每次从特定Partition的特定offset开始fetch特定大小的消息
				完全由Consumer应用程序决定下一次fetch的起始offset
		
			
			适合的情况：
				同一条消息读多次。方便Reolcay
				只消费某个Topic的部分Partition
				管理事务，从而确保每条消息被处理一次，与High Level Consumer相对，Low Level Consumer要求用户做大量的额外工作
				在应用程序中跟踪处理offset，并决定下一条消费那条消息
				获知每个Partition的Leader
				处理Leader的变化
				处理多Consumer的协作
			
		High Level COnsumer：
			自动管理：
				auto.commit.enable=true
				auto.commit.ms60*1000
			手工管理offset：
				ConsumerCOnnector.commitOffsets（）；
			offset存储：
				offsets.storage=zookeeper
				dual.commit/enabled=true
			
			Log compaction：
				一直保持消费Log head的consumer可按顺序消费所有的消息，并且offset连续
				任何从offset 0开始的度操作至少可读到每个key对应的最后一条消息
				每条小的offset保持不变，offset是消息的永久标志符
				消费本身的顺序不会被改变
			
	新API：
		Producer：
			增加发送回调
			重构Partitioner接口
		
		统一High Level APi和Low Level API：
			从kafka.consumer和kafka.javaapi到kafka.clients.consumner
			subscribe动态rebalance vs.assign手动分配
			将offset存于kafka和zookeeper以外
			ConsumerRebalanceListener
			控制消费位置
			消费流程控制
		
		offset管理：
			自动commit

			手动commit：
				手动commit全部offset
				手动commit特定partition的offset
				支持同步和异步commit并支持commit回调

		消费流程控制：
			可暂停/恢复对某些pertitioner的消费
	
	Kafka Stream：
		特点：
			简单轻量的SDK
			除了对Kafka本身的依赖外，无外部依赖
			支持容错的local state从而支持高效率的状态操作，如Join和Window操作
			Record级别的处理
			提供2种处理原语，ProcessorApi和DSL

		Strean Topology：
			Stream时间上无界的，有序的，不可变数据集
			Stream processing application 通过一个或多个Topology定义的计算逻辑
			Stream processor 一个计算原语，类似于Storm的Bolt
		
		Time：
			Event Time消息创建时间，一般由消费携带
			Processing Time消息被处理时的时间
			Ingestion Time消息存入Topic/Partition时的时间
		
		State：
			In-memory State Store
			Persistent State Store

		Window：
			Hopping time windows：
				Advance interval 结果输出interval
				Window size 计算数据集

			Tumbling time windows：
				Hopping time window的特列
			
			Sliding windows：
				只用于join操作，可由JoinWindow类指定

		KStream vs. Ktable：
			KStream：
				数据流，每条消息代表一条不可变的新纪录
			Ktable：
				Ktable为change log流，每条消息代表一个更新，几条key相同的消息会将该key的值更新为最后一条
			Example：
				对于KStream和Ktable中插入2条消息（‘key1’，1），（‘key1’，2）
				对KStream作5sum，结果为（'key1'，3）
				对KTable做sum，结果为（‘key1’，2）

		Join：
			KStream-KStream Join：
				适用于Window join
				结果为KStream
			
			KStream-KTable join：
				KTable的变化值影响KStream的新数据
				新结果的输出由KStream驱动
				输出为KStream
		
			KTable-KTable Join：
				类似于RDMBS的Join
				结果为Ktable
		
		Stream APi：
			


		Kafka Security：
			SSL/TLS

			SASL
			
			
		











		